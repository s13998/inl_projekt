# -*- coding: utf-8 -*-
"""inl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jXTYt2uR9iEsif1HEwmkCi8TOPTGfbb0
"""

import pandas as pd
import numpy as np

# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

train = pd.read_csv('olid-training-v1.0.tsv', sep='\t')
train.head()

test_a = pd.read_csv('testset-levela.tsv', sep='\t')
test_la = pd.read_csv('labels-levela.csv',names=["id","label"])

test_b = pd.read_csv('testset-levelb.tsv', sep='\t')
test_lb = pd.read_csv('labels-levelb.csv',names=["id","label"])

test_tweet_a = test_a.pop('tweet')
test_label_a = test_la.pop('label')

test_tweet_b = test_b.pop('tweet')
test_label_b = test_lb.pop('label')

features_a = train.copy()
labels_a = features_a.pop('subtask_a')
data_a = features_a.pop('tweet')

features_b = train.copy()
features_b = features_b.drop(features_b[features_b.subtask_b != features_b.subtask_b].index)
features_b = features_b.reset_index(drop=True)

labels_b = features_b.pop('subtask_b')
data_b = features_b.pop('tweet')

#22418
VOCAB_SIZE = 20000
encoder_a = tf.keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=VOCAB_SIZE)
encoder_a.adapt(data_a.values)

vocab_a = np.array(encoder_a.get_vocabulary())
vocab_a[:20]

encoded_example_a = encoder_a(data_a.values).numpy()
print(encoded_example_a)

for n in range(5):
  print("Original: ", data_a[n])
  print("Round-trip: ", " ".join(vocab_a[encoded_example_a[n]]))
  print()

VOCAB_SIZE = 10000
encoder_b = tf.keras.layers.experimental.preprocessing.TextVectorization(
    max_tokens=VOCAB_SIZE)
encoder_b.adapt(data_b.values)

vocab_b = np.array(encoder_b.get_vocabulary())
vocab_b[:20]

encoded_example_b = encoder_b(data_b.values).numpy()
print(encoded_example_b[0])

for n in range(5):
  print("Original: ", data_b[n])
  print("Round-trip: ", " ".join(vocab_b[encoded_example_b[n]]))
  print()

labele_a=[]
for i in labels_a.keys():
  if(labels_a[i]=='NOT'):
    labele_a.append(0)
  elif(labels_a[i]=='OFF'):
    labele_a.append(1)
  else:
    print(labels_a[i])

test_labele_a=[]
c=0
for i in range(len(test_label_a)):
  if(test_label_a[i]=='NOT'):
    test_labele_a.append(0)
  elif(test_label_a[i]=='OFF'):
    test_labele_a.append(1)
  else:
    print(test_label_a[i])

labele_b=[]
for i in labels_b.keys():
  if(labels_b[i]=='TIN'):
    labele_b.append(1)
  elif(labels_b[i]=='UNT'):
    labele_b.append(0)
  else:
    print(labels_b[i])

test_labele_b=[]

for i in test_label_b.keys():
  if(test_label_b[i]=='TIN'):
    test_labele_b.append(1)
  elif(test_label_b[i]=='UNT'):
    test_labele_b.append(0)
  else:
    print(test_label_b[i])

model_a = tf.keras.Sequential([
    encoder_a,
    tf.keras.layers.Embedding(
        input_dim=len(encoder_a.get_vocabulary()),
        output_dim=100,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(100),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

model_a.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history_a = model_a.fit(data_a, np.array(labele_a), epochs=10, initial_epoch=0, batch_size=100, 
                    validation_data=(test_tweet_a,np.array(test_labele_a)))

model_b = tf.keras.Sequential([
    encoder_b,
    tf.keras.layers.Embedding(
        input_dim=len(encoder_b.get_vocabulary()),
        output_dim=100,
        #input_length=64,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(100),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

model_b.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history_b = model_b.fit(data_b, np.array(labele_b), epochs=10, initial_epoch=0, batch_size=100, 
                    validation_data=(test_tweet_b,np.array(test_labele_b)))

tmp=model_a.predict(data_a[0:10])

for i in range(10):
  print(tmp[i],' -> ',labele_a[i])

tmp=model_a.predict(test_tweet_a[0:10])

for i in range(10):
  print(tmp[i],' -> ',test_labele_a[i])

tmp=model_b.predict(data_b[0:10])

for i in range(10):
  print(tmp[i],' -> ',labele_b[i])

tmp=model_b.predict(test_tweet_b[0:20])

for i in range(20):
  print(tmp[i],' -> ',test_labele_b[i])

tmp=model_a.predict(test_tweet_a)
tp=1
fp=1
tn=1
fn=1
for i in range(len(tmp)):
  if tmp[i]<0.5:
    if test_labele_a[i]==0:
      tn=tn+1
    else:
      fn=fn+1
  else:
    if test_labele_a[i]==1:
      tp=tp+1
    else:
      fp=fp+1


precisionA = tp/(tp+fp)
precisionB = tn/(tn+fn)

recallA = tp/(tp+fn)
recallB = tn/(tn+fp)

print('precision 1:',precisionA)
print('precision 0:',precisionB)
print('recall 1:',recallA)
print('recall 0:',recallB)

f1A=2*precisionA*recallA/(precisionA+recallA)
f1B=2*precisionB*recallB/(precisionB+recallB)

print('f1 1:',f1A)
print('f1 0:',f1B)

tmp=model_b.predict(test_tweet_b)
tp=1
fp=1
tn=1
fn=1
for i in range(len(tmp)):
  if tmp[i]<0.5:
    if test_labele_b[i]==0:
      tn=tn+1
    else:
      fn=fn+1
  else:
    if test_labele_b[i]==1:
      tp=tp+1
    else:
      fp=fp+1


precisionA = tp/(tp+fp)
precisionB = tn/(tn+fn)

recallA = tp/(tp+fn)
recallB = tn/(tn+fp)

print('precision 1:',precisionA)
print('precision 0:',precisionB)
print('recall 1:',recallA)
print('recall 0:',recallB)

f1A=2*precisionA*recallA/(precisionA+recallA)
f1B=2*precisionB*recallB/(precisionB+recallB)

print('f1 1:',f1A)
print('f1 0:',f1B)

import matplotlib.pyplot as plt


def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plot_graphs(history_a, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(history_a, 'loss')
plt.ylim(0, None)

import matplotlib.pyplot as plt


def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plot_graphs(history_b, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(history_b, 'loss')
plt.ylim(0, None)